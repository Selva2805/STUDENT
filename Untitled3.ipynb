{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b7c53b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = 'model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m emotion_model\u001b[38;5;241m.\u001b[39madd(Dropout(\u001b[38;5;241m0.5\u001b[39m))\n\u001b[0;32m     34\u001b[0m emotion_model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m7\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 35\u001b[0m emotion_model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     37\u001b[0m cv2\u001b[38;5;241m.\u001b[39mocl\u001b[38;5;241m.\u001b[39msetUseOpenCL(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     39\u001b[0m emotion_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAngry\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m1\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDisgusted\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m2\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFearful\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m3\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHappy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m4\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeutral\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m5\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSad\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m6\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSurprised\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\h5py\\_hl\\files.py:567\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    558\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    559\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    560\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    561\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    562\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    563\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    564\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    565\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    566\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 567\u001b[0m     fid \u001b[38;5;241m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[38;5;241m=\u001b[39mswmr)\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\h5py\\_hl\\files.py:231\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    230\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 231\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, flags, fapl\u001b[38;5;241m=\u001b[39mfapl)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    233\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = 'model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from pandastable import Table, TableModel\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import datetime\n",
    "from threading import Thread\n",
    "# from Spotipy import *  \n",
    "import time\n",
    "import pandas as pd\n",
    "face_cascade = cv2.CascadeClassifier(\"C:/path/to/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "ds_factor=0.6\n",
    "\n",
    "emotion_model = Sequential()\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))\n",
    "emotion_model.load_weights('model.h5')\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "emotion_dict = {0:\"Angry\",1:\"Disgusted\",2:\"Fearful\",3:\"Happy\",4:\"Neutral\",5:\"Sad\",6:\"Surprised\"}\n",
    "music_dist={0:\"songs/angry.csv\",1:\"songs/disgusted.csv \",2:\"songs/fearful.csv\",3:\"songs/happy.csv\",4:\"songs/neutral.csv\",5:\"songs/sad.csv\",6:\"songs/surprised.csv\"}\n",
    "global last_frame1                                    \n",
    "last_frame1 = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "global cap1 \n",
    "show_text=[0]\n",
    "\n",
    "\n",
    "''' Class for calculating FPS while streaming. Used this to check performance of using another thread for video streaming '''\n",
    "class FPS:\n",
    "\tdef __init__(self):\n",
    "\t\t# store the start time, end time, and total number of frames\n",
    "\t\t# that were examined between the start and end intervals\n",
    "\t\tself._start = None\n",
    "\t\tself._end = None\n",
    "\t\tself._numFrames = 0\n",
    "\tdef start(self):\n",
    "\t\t# start the timer\n",
    "\t\tself._start = datetime.datetime.now()\n",
    "\t\treturn self\n",
    "\tdef stop(self):\n",
    "\t\t# stop the timer\n",
    "\t\tself._end = datetime.datetime.now()\n",
    "\tdef update(self):\n",
    "\t\t# increment the total number of frames examined during the\n",
    "\t\t# start and end intervals\n",
    "\t\tself._numFrames += 1\n",
    "\tdef elapsed(self):\n",
    "\t\t# return the total number of seconds between the start and\n",
    "\t\t# end interval\n",
    "\t\treturn (self._end - self._start).total_seconds()\n",
    "\tdef fps(self):\n",
    "\t\t# compute the (approximate) frames per second\n",
    "\t\treturn self._numFrames / self.elapsed()\n",
    "\n",
    "\n",
    "''' Class for using another thread for video streaming to boost performance '''\n",
    "class WebcamVideoStream:\n",
    "    \t\n",
    "\t\tdef __init__(self, src=0):\n",
    "\t\t\tself.stream = cv2.VideoCapture(src,cv2.CAP_DSHOW)\n",
    "\t\t\t(self.grabbed, self.frame) = self.stream.read()\n",
    "\t\t\tself.stopped = False\n",
    "\n",
    "\t\tdef start(self):\n",
    "\t\t\t\t# start the thread to read frames from the video stream\n",
    "\t\t\tThread(target=self.update, args=()).start()\n",
    "\t\t\treturn self\n",
    "\t\t\t\n",
    "\t\tdef update(self):\n",
    "\t\t\t# keep looping infinitely until the thread is stopped\n",
    "\t\t\twhile True:\n",
    "\t\t\t\t# if the thread indicator variable is set, stop the thread\n",
    "\t\t\t\tif self.stopped:\n",
    "\t\t\t\t\treturn\n",
    "\t\t\t\t# otherwise, read the next frame from the stream\n",
    "\t\t\t\t(self.grabbed, self.frame) = self.stream.read()\n",
    "\n",
    "\t\tdef read(self):\n",
    "\t\t\t# return the frame most recently read\n",
    "\t\t\treturn self.frame\n",
    "\t\tdef stop(self):\n",
    "\t\t\t# indicate that the thread should be stopped\n",
    "\t\t\tself.stopped = True\n",
    "\n",
    "''' Class for reading video stream, generating prediction and recommendations '''\n",
    "class VideoCamera(object):\n",
    "\t\n",
    "\tdef get_frame(self):\n",
    "\t\tglobal cap1\n",
    "\t\tglobal df1\n",
    "\t\tcap1 = WebcamVideoStream(src=0).start()\n",
    "\t\timage = cap1.read()\n",
    "\t\timage=cv2.resize(image,(600,500))\n",
    "\t\tgray=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\t\tface_rects=face_cascade.detectMultiScale(gray,1.3,5)\n",
    "\t\tdf1 = pd.read_csv(music_dist[show_text[0]])\n",
    "\t\tdf1 = df1[['Name','Album','Artist']]\n",
    "\t\tdf1 = df1.head(15)\n",
    "\t\tfor (x,y,w,h) in face_rects:\n",
    "\t\t\tcv2.rectangle(image,(x,y-50),(x+w,y+h+10),(0,255,0),2)\n",
    "\t\t\troi_gray_frame = gray[y:y + h, x:x + w]\n",
    "\t\t\tcropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "\t\t\tprediction = emotion_model.predict(cropped_img)\n",
    "\n",
    "\t\t\tmaxindex = int(np.argmax(prediction))\n",
    "\t\t\tshow_text[0] = maxindex \n",
    "\t\t\t#print(\"===========================================\",music_dist[show_text[0]],\"===========================================\")\n",
    "\t\t\t#print(df1)\n",
    "\t\t\tcv2.putText(image, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\t\t\tdf1 = music_rec()\n",
    "\t\t\t\n",
    "\t\tglobal last_frame1\n",
    "\t\tlast_frame1 = image.copy()\n",
    "\t\tpic = cv2.cvtColor(last_frame1, cv2.COLOR_BGR2RGB)     \n",
    "\t\timg = Image.fromarray(last_frame1)\n",
    "\t\timg = np.array(img)\n",
    "\t\tret, jpeg = cv2.imencode('.jpg', img)\n",
    "\t\treturn jpeg.tobytes(), df1\n",
    "\n",
    "def music_rec():\n",
    "\t# print('---------------- Value ------------', music_dist[show_text[0]])\n",
    "\tdf = pd.read_csv(music_dist[show_text[0]])\n",
    "\tdf = df[['Name','Album','Artist']]\n",
    "\tdf = df.head(15)\n",
    "\treturn df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b939f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
